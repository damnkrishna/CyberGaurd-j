so now after gaining some basic knowledge regarding ai/ml and dataset i am thinking of seeing the dataset provided by datahackathon of aadhar data of some years with only numerical data like new enrollment , demograhic update and biometric update type stuff log and using that type of data we have identify technique to find anomality and fraud happening in some place or locality using the dataset of past years  well the dataset is too large it is around 30 lakh rows and 8 columns of data 

well last time i didnt have any idea what exactly is the use of these dataset can be but now atleast ik a little ik there is possibility of finding stuff using these dataset 

welll its not much about the model thought we did choose 3 specific technique z-score, isolation forest and cross-dataset for grading or putting them in prioty based upon their value of total detection using weighted values 

also based upn these risk-scoring analysis still instead of comparing all 900 of distrcit in top prioty we deicded to mark them or flag them high/medium/low and also based upon decided value we choosed top 5 district to anlyse a little deeper and anylse them based upon fax and our detection and also suggesting improvement in these distrcit for both security and data improvement we basically suggested ways and technique to secure data and flag them when risk is detected using ml model and different feature engineering technique
